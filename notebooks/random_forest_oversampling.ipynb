{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>Y0</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Y3</th>\n",
       "      <th>Y4</th>\n",
       "      <th>Y5</th>\n",
       "      <th>Y6</th>\n",
       "      <th>Y7</th>\n",
       "      <th>Y8</th>\n",
       "      <th>Y9</th>\n",
       "      <th>Z0</th>\n",
       "      <th>Z1</th>\n",
       "      <th>Z2</th>\n",
       "      <th>Z3</th>\n",
       "      <th>Z4</th>\n",
       "      <th>Z5</th>\n",
       "      <th>Z6</th>\n",
       "      <th>Z7</th>\n",
       "      <th>Z8</th>\n",
       "      <th>Z9</th>\n",
       "      <th>XAVG</th>\n",
       "      <th>YAVG</th>\n",
       "      <th>ZAVG</th>\n",
       "      <th>XPEAK</th>\n",
       "      <th>YPEAK</th>\n",
       "      <th>ZPEAK</th>\n",
       "      <th>XABSOLDEV</th>\n",
       "      <th>YABSOLDEV</th>\n",
       "      <th>ZABSOLDEV</th>\n",
       "      <th>XSTANDDEV</th>\n",
       "      <th>YSTANDDEV</th>\n",
       "      <th>ZSTANDDEV</th>\n",
       "      <th>RESULTANT</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2075.00</td>\n",
       "      <td>293.94</td>\n",
       "      <td>1550.00</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.21</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>8.17</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11.96</td>\n",
       "      <td>Jogging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0</td>\n",
       "      <td>7.62</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1525.00</td>\n",
       "      <td>269.44</td>\n",
       "      <td>1233.33</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.05</td>\n",
       "      <td>5.43</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.43</td>\n",
       "      <td>12.05</td>\n",
       "      <td>Jogging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0</td>\n",
       "      <td>7.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>1766.67</td>\n",
       "      <td>248.65</td>\n",
       "      <td>1780.00</td>\n",
       "      <td>4.18</td>\n",
       "      <td>6.89</td>\n",
       "      <td>4.07</td>\n",
       "      <td>5.55</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.55</td>\n",
       "      <td>11.99</td>\n",
       "      <td>Jogging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2525.00</td>\n",
       "      <td>709.09</td>\n",
       "      <td>1380.00</td>\n",
       "      <td>2.26</td>\n",
       "      <td>4.13</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.87</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.87</td>\n",
       "      <td>10.69</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "      <td>9.76</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1075.00</td>\n",
       "      <td>3300.00</td>\n",
       "      <td>1775.00</td>\n",
       "      <td>2.29</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.41</td>\n",
       "      <td>3.08</td>\n",
       "      <td>4.64</td>\n",
       "      <td>3.08</td>\n",
       "      <td>10.80</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0</td>\n",
       "      <td>7.73</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3125.00</td>\n",
       "      <td>1337.50</td>\n",
       "      <td>2675.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.13</td>\n",
       "      <td>4.95</td>\n",
       "      <td>3.13</td>\n",
       "      <td>8.63</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0</td>\n",
       "      <td>9.20</td>\n",
       "      <td>1.19</td>\n",
       "      <td>4900.00</td>\n",
       "      <td>1058.33</td>\n",
       "      <td>1825.00</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.09</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.09</td>\n",
       "      <td>9.87</td>\n",
       "      <td>Upstairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "      <td>9.30</td>\n",
       "      <td>1.40</td>\n",
       "      <td>800.00</td>\n",
       "      <td>1275.00</td>\n",
       "      <td>2983.33</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2.22</td>\n",
       "      <td>9.91</td>\n",
       "      <td>Upstairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0</td>\n",
       "      <td>9.06</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1283.33</td>\n",
       "      <td>845.45</td>\n",
       "      <td>1533.33</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.78</td>\n",
       "      <td>Upstairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.85</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.39</td>\n",
       "      <td>Upstairs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user    X0    X1    X2    X3    X4    X5    X6    X7    X8    X9    Y0  \\\n",
       "0    33  0.04  0.09  0.14  0.12  0.11  0.10  0.08  0.13  0.13  0.08  0.09   \n",
       "1    33  0.12  0.12  0.06  0.07  0.11  0.10  0.11  0.09  0.12  0.10  0.12   \n",
       "2    33  0.14  0.09  0.11  0.09  0.09  0.11  0.12  0.08  0.05  0.12  0.09   \n",
       "3    33  0.06  0.10  0.09  0.09  0.11  0.07  0.12  0.10  0.14  0.14  0.12   \n",
       "4    33  0.12  0.11  0.10  0.08  0.10  0.14  0.10  0.11  0.08  0.07  0.10   \n",
       "5    33  0.09  0.09  0.10  0.12  0.08  0.06  0.09  0.08  0.07  0.08  0.07   \n",
       "6    33  0.12  0.12  0.12  0.13  0.15  0.11  0.08  0.09  0.06  0.03  0.10   \n",
       "7    33  0.10  0.10  0.10  0.10  0.11  0.11  0.09  0.09  0.12  0.12  0.09   \n",
       "8    33  0.08  0.07  0.08  0.08  0.05  0.08  0.14  0.10  0.15  0.16  0.12   \n",
       "9    33  0.01  0.00  0.00  0.01  0.01  0.01  0.01  0.01  0.00  0.00  0.00   \n",
       "\n",
       "     Y1    Y2    Y3    Y4    Y5    Y6    Y7    Y8    Y9    Z0    Z1    Z2  \\\n",
       "0  0.10  0.11  0.11  0.08  0.04  0.16  0.13  0.10  0.03  0.12  0.08  0.09   \n",
       "1  0.11  0.07  0.10  0.13  0.13  0.06  0.11  0.10  0.04  0.11  0.11  0.11   \n",
       "2  0.10  0.12  0.10  0.10  0.12  0.08  0.07  0.10  0.06  0.07  0.11  0.10   \n",
       "3  0.08  0.11  0.10  0.10  0.11  0.11  0.10  0.08  0.11  0.10  0.10  0.11   \n",
       "4  0.11  0.07  0.09  0.10  0.11  0.10  0.11  0.14  0.09  0.10  0.09  0.11   \n",
       "5  0.10  0.11  0.08  0.08  0.08  0.09  0.08  0.07  0.09  0.08  0.10  0.06   \n",
       "6  0.11  0.09  0.10  0.10  0.12  0.09  0.11  0.10  0.10  0.13  0.08  0.11   \n",
       "7  0.11  0.10  0.08  0.10  0.12  0.08  0.10  0.11  0.11  0.08  0.11  0.10   \n",
       "8  0.09  0.09  0.12  0.11  0.08  0.11  0.09  0.10  0.10  0.08  0.09  0.10   \n",
       "9  0.00  0.00  0.01  0.01  0.01  0.02  0.01  0.00  0.00  0.01  0.02  0.00   \n",
       "\n",
       "     Z3    Z4    Z5    Z6    Z7    Z8    Z9  XAVG  YAVG  ZAVG    XPEAK  \\\n",
       "0  0.12  0.10  0.10  0.08  0.11  0.12  0.10     0  8.40  1.76  2075.00   \n",
       "1  0.09  0.12  0.10  0.11  0.10  0.07  0.08     0  7.62  1.43  1525.00   \n",
       "2  0.09  0.08  0.11  0.11  0.09  0.10  0.12     0  7.77  2.39  1766.67   \n",
       "3  0.10  0.09  0.07  0.12  0.14  0.07  0.11     0  9.57  0.49  2525.00   \n",
       "4  0.10  0.11  0.11  0.08  0.09  0.14  0.10     0  9.76  0.51  1075.00   \n",
       "5  0.09  0.09  0.11  0.08  0.08  0.07  0.08     0  7.73  0.31  3125.00   \n",
       "6  0.12  0.08  0.07  0.09  0.10  0.11  0.13     0  9.20  1.19  4900.00   \n",
       "7  0.10  0.12  0.10  0.13  0.10  0.07  0.10     0  9.30  1.40   800.00   \n",
       "8  0.10  0.10  0.11  0.11  0.12  0.12  0.08     0  9.06  1.38  1283.33   \n",
       "9  0.01  0.00  0.00  0.01  0.00  0.00  0.00     0  0.39 -0.01      NaN   \n",
       "\n",
       "     YPEAK    ZPEAK  XABSOLDEV  YABSOLDEV  ZABSOLDEV  XSTANDDEV  YSTANDDEV  \\\n",
       "0   293.94  1550.00       3.29       7.21       4.00       4.05       8.17   \n",
       "1   269.44  1233.33       4.23       6.88       4.05       5.43       8.19   \n",
       "2   248.65  1780.00       4.18       6.89       4.07       5.55       8.19   \n",
       "3   709.09  1380.00       2.26       4.13       2.49       2.87       4.95   \n",
       "4  3300.00  1775.00       2.29       3.94       2.41       3.08       4.64   \n",
       "5  1337.50  2675.00       2.00       3.39       1.94       3.13       4.95   \n",
       "6  1058.33  1825.00       1.56       2.55       1.72       2.09       3.20   \n",
       "7  1275.00  2983.33       1.84       2.69       1.41       2.22       3.35   \n",
       "8   845.45  1533.33       2.26       2.70       1.30       2.68       3.40   \n",
       "9   125.00      NaN       0.07       0.37       0.03       1.85       9.34   \n",
       "\n",
       "   ZSTANDDEV  RESULTANT     class  \n",
       "0       4.05      11.96   Jogging  \n",
       "1       5.43      12.05   Jogging  \n",
       "2       5.55      11.99   Jogging  \n",
       "3       2.87      10.69   Walking  \n",
       "4       3.08      10.80   Walking  \n",
       "5       3.13       8.63   Walking  \n",
       "6       2.09       9.87  Upstairs  \n",
       "7       2.22       9.91  Upstairs  \n",
       "8       2.68       9.78  Upstairs  \n",
       "9       1.85       0.39  Upstairs  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/WISDM_ar_v1.1_transformed.csv', na_values=[\"?\"])\n",
    "df.drop([\"UNIQUE_ID\"],axis=1, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5418, 45)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Number Values in each Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df[\"class\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df[\"user\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform non-numerical labels to numerical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Downstairs', 'Jogging', 'Sitting', 'Standing', 'Upstairs',\n",
       "       'Walking'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clEnc = LabelEncoder()\n",
    "df[\"class\"] = clEnc.fit_transform(df[\"class\"])\n",
    "clEnc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting corrilation of each atributes with Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.138669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XAVG</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YAVG</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.304911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZAVG</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.109477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XPEAK</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.132298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YPEAK</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.195308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZPEAK</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.119971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XABSOLDEV</th>\n",
       "      <td>-0.109400</td>\n",
       "      <td>-0.234542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YABSOLDEV</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.379568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZABSOLDEV</th>\n",
       "      <td>-0.110352</td>\n",
       "      <td>-0.263535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XSTANDDEV</th>\n",
       "      <td>-0.119288</td>\n",
       "      <td>-0.277638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YSTANDDEV</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.393652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZSTANDDEV</th>\n",
       "      <td>-0.119288</td>\n",
       "      <td>-0.277638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RESULTANT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.207198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user     class\n",
       "user       1.000000       NaN\n",
       "X0              NaN       NaN\n",
       "X1              NaN       NaN\n",
       "X2              NaN       NaN\n",
       "X3              NaN       NaN\n",
       "X4              NaN       NaN\n",
       "X5              NaN       NaN\n",
       "X6              NaN       NaN\n",
       "X7              NaN       NaN\n",
       "X8              NaN       NaN\n",
       "X9              NaN       NaN\n",
       "Y0              NaN       NaN\n",
       "Y1              NaN       NaN\n",
       "Y2              NaN       NaN\n",
       "Y3              NaN       NaN\n",
       "Y4              NaN       NaN\n",
       "Y5              NaN       NaN\n",
       "Y6              NaN       NaN\n",
       "Y7              NaN       NaN\n",
       "Y8              NaN       NaN\n",
       "Y9              NaN  0.138669\n",
       "Z0              NaN       NaN\n",
       "Z1              NaN       NaN\n",
       "Z2              NaN       NaN\n",
       "Z3              NaN       NaN\n",
       "Z4              NaN       NaN\n",
       "Z5              NaN       NaN\n",
       "Z6              NaN       NaN\n",
       "Z7              NaN       NaN\n",
       "Z8              NaN       NaN\n",
       "Z9              NaN       NaN\n",
       "XAVG            NaN       NaN\n",
       "YAVG            NaN  0.304911\n",
       "ZAVG            NaN -0.109477\n",
       "XPEAK           NaN  0.132298\n",
       "YPEAK           NaN  0.195308\n",
       "ZPEAK           NaN  0.119971\n",
       "XABSOLDEV -0.109400 -0.234542\n",
       "YABSOLDEV       NaN -0.379568\n",
       "ZABSOLDEV -0.110352 -0.263535\n",
       "XSTANDDEV -0.119288 -0.277638\n",
       "YSTANDDEV       NaN -0.393652\n",
       "ZSTANDDEV -0.119288 -0.277638\n",
       "RESULTANT       NaN -0.207198\n",
       "class           NaN  1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_df = df.corr()[['user', 'class']] \n",
    "corr_df[((corr_df > 0.1) | (corr_df < -0.1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['XPEAK'] = np.log(df['XPEAK'])\n",
    "df['YPEAK'] = np.log(df['YPEAK'])\n",
    "df['ZPEAK'] = np.log(df['ZPEAK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filling na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['XPEAK'] = df['XPEAK'].fillna(df['XPEAK'].mean())\n",
    "df['YPEAK'] = df['YPEAK'].fillna(df['YPEAK'].mean())\n",
    "df['ZPEAK'] = df['ZPEAK'].fillna(df['ZPEAK'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spliting data into Dependent (X) and Independent (Y) Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "X = df.drop([\"user\", \"class\"], 1)\n",
    "# Y = df[[\"user\", \"class\"]]\n",
    "Y = df[[\"class\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 has 528 instances\n",
      "Class 1 has 1625 instances\n",
      "Class 2 has 306 instances\n",
      "Class 3 has 246 instances\n",
      "Class 4 has 632 instances\n",
      "Class 5 has 2081 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 has 2081 instances after oversampling\n",
      "Class 1 has 2081 instances after oversampling\n",
      "Class 2 has 2081 instances after oversampling\n",
      "Class 3 has 2080 instances after oversampling\n",
      "Class 4 has 2081 instances after oversampling\n",
      "Class 5 has 2081 instances after oversampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from kmeans_smote import KMeansSMOTE\n",
    "\n",
    "[print('Class {} has {} instances'.format(label, count))\n",
    " for label, count in zip(*np.unique(Y, return_counts=True))]\n",
    "\n",
    "kmeans_smote = KMeansSMOTE(\n",
    "    kmeans_args={\n",
    "        'n_clusters': 100\n",
    "    },\n",
    "    smote_args={\n",
    "        'k_neighbors': 10\n",
    "    }\n",
    ")\n",
    "X_resampled, y_resampled = kmeans_smote.fit_sample(X, Y)\n",
    "\n",
    "[print('Class {} has {} instances after oversampling'.format(label, count))\n",
    " for label, count in zip(*np.unique(y_resampled, return_counts=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = pd.DataFrame(y_resampled, columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12480</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12483</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12484</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12485 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class\n",
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          5\n",
       "4          5\n",
       "...      ...\n",
       "12480      4\n",
       "12481      4\n",
       "12482      4\n",
       "12483      4\n",
       "12484      4\n",
       "\n",
       "[12485 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12485, 43)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.833319983980777,\n",
       " 4: 0.833319983980777,\n",
       " 2: 0.833319983980777,\n",
       " 1: 0.833319983980777,\n",
       " 0: 0.833319983980777,\n",
       " 3: 0.8334000800961153}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(1- yy['class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X_resampled, y_resampled, test_size=.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 5, ..., 4, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507408890668803"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Train the model\n",
    "csf = RandomForestClassifier(n_estimators=500, class_weight= dict(1- yy['class'].value_counts(normalize=True)))\n",
    "csf.fit(trainX, trainY)\n",
    "pred = csf.predict(testX)\n",
    "\n",
    "y_test = pd.DataFrame(testY, columns=['class'])[['class']]\n",
    "y_pred = pd.DataFrame(pred, columns=['class'])[['class']]\n",
    "\n",
    "#Evaluate\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507408890668803"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87438424 0.99285714 0.99519231 0.99022005 0.88372093 0.96875   ]\n",
      "[0.93915344 0.97429907 1.         1.         0.94527363 0.85744681]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.90561224, 0.98349057, 0.99759036, 0.995086  , 0.91346154,\n",
       "       0.90970655])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_test, y_pred, average=None))\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "# F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>1.76</td>\n",
       "      <td>7.637716</td>\n",
       "      <td>5.683376</td>\n",
       "      <td>7.346010</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.21</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>8.17</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.62</td>\n",
       "      <td>1.43</td>\n",
       "      <td>7.329750</td>\n",
       "      <td>5.596346</td>\n",
       "      <td>7.117473</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.05</td>\n",
       "      <td>5.43</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.43</td>\n",
       "      <td>12.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>7.476852</td>\n",
       "      <td>5.516046</td>\n",
       "      <td>7.484369</td>\n",
       "      <td>4.18</td>\n",
       "      <td>6.89</td>\n",
       "      <td>4.07</td>\n",
       "      <td>5.55</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.55</td>\n",
       "      <td>11.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.49</td>\n",
       "      <td>7.833996</td>\n",
       "      <td>6.563982</td>\n",
       "      <td>7.229839</td>\n",
       "      <td>2.26</td>\n",
       "      <td>4.13</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.87</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.87</td>\n",
       "      <td>10.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.76</td>\n",
       "      <td>0.51</td>\n",
       "      <td>6.980076</td>\n",
       "      <td>8.101678</td>\n",
       "      <td>7.481556</td>\n",
       "      <td>2.29</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.41</td>\n",
       "      <td>3.08</td>\n",
       "      <td>4.64</td>\n",
       "      <td>3.08</td>\n",
       "      <td>10.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9    10    11  \\\n",
       "0  0.04  0.09  0.14  0.12  0.11  0.10  0.08  0.13  0.13  0.08  0.09  0.10   \n",
       "1  0.12  0.12  0.06  0.07  0.11  0.10  0.11  0.09  0.12  0.10  0.12  0.11   \n",
       "2  0.14  0.09  0.11  0.09  0.09  0.11  0.12  0.08  0.05  0.12  0.09  0.10   \n",
       "3  0.06  0.10  0.09  0.09  0.11  0.07  0.12  0.10  0.14  0.14  0.12  0.08   \n",
       "4  0.12  0.11  0.10  0.08  0.10  0.14  0.10  0.11  0.08  0.07  0.10  0.11   \n",
       "\n",
       "     12    13    14    15    16    17    18    19    20    21    22    23  \\\n",
       "0  0.11  0.11  0.08  0.04  0.16  0.13  0.10  0.03  0.12  0.08  0.09  0.12   \n",
       "1  0.07  0.10  0.13  0.13  0.06  0.11  0.10  0.04  0.11  0.11  0.11  0.09   \n",
       "2  0.12  0.10  0.10  0.12  0.08  0.07  0.10  0.06  0.07  0.11  0.10  0.09   \n",
       "3  0.11  0.10  0.10  0.11  0.11  0.10  0.08  0.11  0.10  0.10  0.11  0.10   \n",
       "4  0.07  0.09  0.10  0.11  0.10  0.11  0.14  0.09  0.10  0.09  0.11  0.10   \n",
       "\n",
       "     24    25    26    27    28    29   30    31    32        33        34  \\\n",
       "0  0.10  0.10  0.08  0.11  0.12  0.10  0.0  8.40  1.76  7.637716  5.683376   \n",
       "1  0.12  0.10  0.11  0.10  0.07  0.08  0.0  7.62  1.43  7.329750  5.596346   \n",
       "2  0.08  0.11  0.11  0.09  0.10  0.12  0.0  7.77  2.39  7.476852  5.516046   \n",
       "3  0.09  0.07  0.12  0.14  0.07  0.11  0.0  9.57  0.49  7.833996  6.563982   \n",
       "4  0.11  0.11  0.08  0.09  0.14  0.10  0.0  9.76  0.51  6.980076  8.101678   \n",
       "\n",
       "         35    36    37    38    39    40    41     42  \n",
       "0  7.346010  3.29  7.21  4.00  4.05  8.17  4.05  11.96  \n",
       "1  7.117473  4.23  6.88  4.05  5.43  8.19  5.43  12.05  \n",
       "2  7.484369  4.18  6.89  4.07  5.55  8.19  5.55  11.99  \n",
       "3  7.229839  2.26  4.13  2.49  2.87  4.95  2.87  10.69  \n",
       "4  7.481556  2.29  3.94  2.41  3.08  4.64  3.08  10.80  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_over = pd.DataFrame(X_resampled)\n",
    "df_over.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over['class'] = pd.DataFrame(y_resampled, columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>1.76</td>\n",
       "      <td>7.637716</td>\n",
       "      <td>5.683376</td>\n",
       "      <td>7.346010</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.21</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>8.17</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.62</td>\n",
       "      <td>1.43</td>\n",
       "      <td>7.329750</td>\n",
       "      <td>5.596346</td>\n",
       "      <td>7.117473</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.05</td>\n",
       "      <td>5.43</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.43</td>\n",
       "      <td>12.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>7.476852</td>\n",
       "      <td>5.516046</td>\n",
       "      <td>7.484369</td>\n",
       "      <td>4.18</td>\n",
       "      <td>6.89</td>\n",
       "      <td>4.07</td>\n",
       "      <td>5.55</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.55</td>\n",
       "      <td>11.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.49</td>\n",
       "      <td>7.833996</td>\n",
       "      <td>6.563982</td>\n",
       "      <td>7.229839</td>\n",
       "      <td>2.26</td>\n",
       "      <td>4.13</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.87</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.87</td>\n",
       "      <td>10.69</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.76</td>\n",
       "      <td>0.51</td>\n",
       "      <td>6.980076</td>\n",
       "      <td>8.101678</td>\n",
       "      <td>7.481556</td>\n",
       "      <td>2.29</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.41</td>\n",
       "      <td>3.08</td>\n",
       "      <td>4.64</td>\n",
       "      <td>3.08</td>\n",
       "      <td>10.80</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9    10    11  \\\n",
       "0  0.04  0.09  0.14  0.12  0.11  0.10  0.08  0.13  0.13  0.08  0.09  0.10   \n",
       "1  0.12  0.12  0.06  0.07  0.11  0.10  0.11  0.09  0.12  0.10  0.12  0.11   \n",
       "2  0.14  0.09  0.11  0.09  0.09  0.11  0.12  0.08  0.05  0.12  0.09  0.10   \n",
       "3  0.06  0.10  0.09  0.09  0.11  0.07  0.12  0.10  0.14  0.14  0.12  0.08   \n",
       "4  0.12  0.11  0.10  0.08  0.10  0.14  0.10  0.11  0.08  0.07  0.10  0.11   \n",
       "\n",
       "     12    13    14    15    16    17    18    19    20    21    22    23  \\\n",
       "0  0.11  0.11  0.08  0.04  0.16  0.13  0.10  0.03  0.12  0.08  0.09  0.12   \n",
       "1  0.07  0.10  0.13  0.13  0.06  0.11  0.10  0.04  0.11  0.11  0.11  0.09   \n",
       "2  0.12  0.10  0.10  0.12  0.08  0.07  0.10  0.06  0.07  0.11  0.10  0.09   \n",
       "3  0.11  0.10  0.10  0.11  0.11  0.10  0.08  0.11  0.10  0.10  0.11  0.10   \n",
       "4  0.07  0.09  0.10  0.11  0.10  0.11  0.14  0.09  0.10  0.09  0.11  0.10   \n",
       "\n",
       "     24    25    26    27    28    29   30    31    32        33        34  \\\n",
       "0  0.10  0.10  0.08  0.11  0.12  0.10  0.0  8.40  1.76  7.637716  5.683376   \n",
       "1  0.12  0.10  0.11  0.10  0.07  0.08  0.0  7.62  1.43  7.329750  5.596346   \n",
       "2  0.08  0.11  0.11  0.09  0.10  0.12  0.0  7.77  2.39  7.476852  5.516046   \n",
       "3  0.09  0.07  0.12  0.14  0.07  0.11  0.0  9.57  0.49  7.833996  6.563982   \n",
       "4  0.11  0.11  0.08  0.09  0.14  0.10  0.0  9.76  0.51  6.980076  8.101678   \n",
       "\n",
       "         35    36    37    38    39    40    41     42  class  \n",
       "0  7.346010  3.29  7.21  4.00  4.05  8.17  4.05  11.96      1  \n",
       "1  7.117473  4.23  6.88  4.05  5.43  8.19  5.43  12.05      1  \n",
       "2  7.484369  4.18  6.89  4.07  5.55  8.19  5.55  11.99      1  \n",
       "3  7.229839  2.26  4.13  2.49  2.87  4.95  2.87  10.69      5  \n",
       "4  7.481556  2.29  3.94  2.41  3.08  4.64  3.08  10.80      5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_over.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Class Over sampling - 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf5 = KFold(n_splits = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     3 ... 12481 12482 12484] [    2     5     7 ... 12474 12478 12483]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 1 on the test set: 0.9499399279134962\n",
      "[    0     1     2 ... 12482 12483 12484] [    6    14    20 ... 12456 12469 12473]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 2 on the test set: 0.9495394473368042\n",
      "[    0     2     3 ... 12481 12483 12484] [    1     8    17 ... 12470 12476 12482]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 3 on the test set: 0.9547456948338006\n",
      "[    1     2     3 ... 12482 12483 12484] [    0    12    13 ... 12472 12475 12479]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 4 on the test set: 0.9515418502202643\n",
      "[    0     1     2 ... 12479 12482 12483] [    3     4    10 ... 12480 12481 12484]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 5 on the test set: 0.9527432919503404\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scores = []\n",
    "i = 1\n",
    "for train_index, test_index in kf5.split(df_over):\n",
    "    print(train_index, test_index)\n",
    "    trainX = df_over.iloc[train_index].drop([\"class\"], 1)\n",
    "    testX = df_over.iloc[test_index].drop([\"class\"], 1)\n",
    "    trainY = df_over.iloc[train_index][[\"class\"]]\n",
    "    testY = df_over.loc[test_index][[\"class\"]]\n",
    "    \n",
    "    #Train the model\n",
    "    csf = RandomForestClassifier(n_estimators=500, class_weight= dict(1- trainY['class'].value_counts(normalize=True)))\n",
    "    csf.fit(trainX, trainY)\n",
    "    pred = csf.predict(testX)\n",
    "\n",
    "    y_test = testY[['class']]\n",
    "    y_pred = pd.DataFrame(pred, columns=['class'])[['class']]\n",
    "    \n",
    "    #Evaluate\n",
    "    print(f\"Accuracy for the fold no. {i} on the test set: {accuracy_score(y_test, y_pred)}\")\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9499399279134962,\n",
       " 0.9495394473368042,\n",
       " 0.9547456948338006,\n",
       " 0.9515418502202643,\n",
       " 0.9527432919503404]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9517020424509411"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold User-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf5 = KFold(n_splits = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 5415 5416 5417] [   3    5    7 ... 5401 5403 5408]\n",
      "Accuracy for the fold no. 1 on the test set: 0.9012915129151291\n",
      "[   1    3    4 ... 5414 5415 5416] [   0    2    6 ... 5412 5413 5417]\n",
      "Accuracy for the fold no. 2 on the test set: 0.9040590405904059\n",
      "[   0    2    3 ... 5415 5416 5417] [   1    8   10 ... 5398 5400 5407]\n",
      "Accuracy for the fold no. 3 on the test set: 0.8939114391143912\n",
      "[   0    1    2 ... 5414 5416 5417] [  17   18   20 ... 5404 5409 5415]\n",
      "Accuracy for the fold no. 4 on the test set: 0.8975069252077562\n",
      "[   0    1    2 ... 5413 5415 5417] [   4    9   12 ... 5406 5414 5416]\n",
      "Accuracy for the fold no. 5 on the test set: 0.889196675900277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scores = []\n",
    "i = 1\n",
    "for train_index, test_index in kf5.split(df):\n",
    "    print(train_index, test_index)\n",
    "    trainX = df.iloc[train_index].drop([\"user\", \"class\"], 1)\n",
    "    testX = df.iloc[test_index].drop([\"user\", \"class\"], 1)\n",
    "    trainY = df.iloc[train_index][[\"user\", \"class\"]]\n",
    "    testY = df.loc[test_index][[\"user\", \"class\"]]\n",
    "    \n",
    "    #Train the model\n",
    "    csf = RandomForestClassifier(n_estimators=500, class_weight=[dict(1 - trainY['user'].value_counts(normalize=True)), \n",
    "                                                                 dict(1- trainY['class'].value_counts(normalize=True))])\n",
    "    csf.fit(trainX, trainY)\n",
    "    pred = csf.predict(testX)\n",
    "\n",
    "    y_test = testY[['class']]\n",
    "    y_pred = pd.DataFrame(pred, columns=['user', 'class'])[['class']]\n",
    "    \n",
    "    #Evaluate\n",
    "    print(f\"Accuracy for the fold no. {i} on the test set: {accuracy_score(y_test, y_pred)}\")\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8971931187455918"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf5 = KFold(n_splits = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 5413 5416 5417] [   4    6    7 ... 5397 5414 5415]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 1 on the test set: 0.8985239852398524\n",
      "[   0    1    2 ... 5415 5416 5417] [   5   10   11 ... 5403 5404 5409]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 2 on the test set: 0.8994464944649446\n",
      "[   0    1    3 ... 5414 5415 5417] [   2    9   14 ... 5396 5405 5416]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 3 on the test set: 0.8892988929889298\n",
      "[   2    3    4 ... 5414 5415 5416] [   0    1   17 ... 5412 5413 5417]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 4 on the test set: 0.8744228993536473\n",
      "[   0    1    2 ... 5415 5416 5417] [   3   12   19 ... 5394 5406 5408]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 5 on the test set: 0.8919667590027701\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scores = []\n",
    "i = 1\n",
    "for train_index, test_index in kf5.split(df):\n",
    "    print(train_index, test_index)\n",
    "    trainX = df.iloc[train_index].drop([\"user\", \"class\"], 1)\n",
    "    testX = df.iloc[test_index].drop([\"user\", \"class\"], 1)\n",
    "    trainY = df.iloc[train_index][[\"class\"]]\n",
    "    testY = df.loc[test_index][[\"class\"]]\n",
    "    \n",
    "    #Train the model\n",
    "    csf = RandomForestClassifier(n_estimators=500, class_weight= dict(1- trainY['class'].value_counts(normalize=True)))\n",
    "    csf.fit(trainX, trainY)\n",
    "    pred = csf.predict(testX)\n",
    "\n",
    "    y_test = testY[['class']]\n",
    "    y_pred = pd.DataFrame(pred, columns=['class'])[['class']]\n",
    "    \n",
    "    #Evaluate\n",
    "    print(f\"Accuracy for the fold no. {i} on the test set: {accuracy_score(y_test, y_pred)}\")\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8907318062100289"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User, Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 1 on the test set: 0.8948339483394834\n",
      "Accuracy for the fold no. 2 on the test set: 0.8948339483394834\n",
      "Accuracy for the fold no. 3 on the test set: 0.8819188191881919\n",
      "Accuracy for the fold no. 4 on the test set: 0.9003690036900369\n",
      "Accuracy for the fold no. 5 on the test set: 0.8911439114391144\n",
      "Accuracy for the fold no. 6 on the test set: 0.9261992619926199\n",
      "Accuracy for the fold no. 7 on the test set: 0.9298892988929889\n",
      "Accuracy for the fold no. 8 on the test set: 0.9169741697416974\n",
      "Accuracy for the fold no. 9 on the test set: 0.9279112754158965\n",
      "Accuracy for the fold no. 10 on the test set: 0.8946395563770795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scores = []\n",
    "\n",
    "kf10 = KFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in kf10.split(df):\n",
    "    trainX = df.iloc[train_index].drop([\"user\", \"class\"], 1)\n",
    "    testX = df.iloc[test_index].drop([\"user\", \"class\"], 1)\n",
    "    trainY = df.iloc[train_index][[\"user\", \"class\"]]\n",
    "    testY = df.loc[test_index][[\"user\", \"class\"]]\n",
    "    \n",
    "    #Train the model\n",
    "    csf = RandomForestClassifier(n_estimators=500, class_weight=[dict(1 - trainY['user'].value_counts(normalize=True)), \n",
    "                                                                 dict(1- trainY['class'].value_counts(normalize=True))])\n",
    "    csf.fit(trainX, trainY)\n",
    "    pred = csf.predict(testX)\n",
    "\n",
    "    y_test = testY[['class']]\n",
    "    y_pred = pd.DataFrame(pred, columns=['user', 'class'])[['class']]\n",
    "    \n",
    "    #Evaluate\n",
    "    print(f\"Accuracy for the fold no. {i} on the test set: {accuracy_score(y_test, y_pred)}\")\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8819188191881919"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(scores) # n_estimators  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 1 on the test set: 0.8966789667896679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 2 on the test set: 0.8929889298892989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 3 on the test set: 0.8929889298892989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 4 on the test set: 0.8929889298892989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 5 on the test set: 0.8911439114391144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 6 on the test set: 0.8874538745387454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 7 on the test set: 0.8985239852398524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 8 on the test set: 0.8892988929889298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 9 on the test set: 0.9038817005545287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 10 on the test set: 0.9075785582255084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scores2 = []\n",
    "\n",
    "kf10 = KFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in kf10.split(df):\n",
    "    trainX = df.iloc[train_index].drop([\"user\", \"class\"], 1)\n",
    "    testX = df.iloc[test_index].drop([\"user\", \"class\"], 1)\n",
    "    trainY = df.iloc[train_index][[\"class\"]]\n",
    "    testY = df.loc[test_index][[\"class\"]]\n",
    "    \n",
    "    #Train the model\n",
    "    csf = RandomForestClassifier(n_estimators=500, class_weight=dict(1- trainY['class'].value_counts(normalize=True)))\n",
    "    csf.fit(trainX, trainY)\n",
    "    pred = csf.predict(testX)\n",
    "\n",
    "    y_test = testY[['class']]\n",
    "    y_pred = pd.DataFrame(pred, columns=['class'])[['class']]\n",
    "    \n",
    "    #Evaluate\n",
    "    print(f\"Accuracy for the fold no. {i} on the test set: {accuracy_score(y_test, y_pred)}\")\n",
    "    scores2.append(accuracy_score(y_test, y_pred))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8874538745387454"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(scores2) # n_estimators  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9075785582255084\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 33   0   0   0  13  13]\n",
      " [  2 158   0   0   2   1]\n",
      " [  1   0  35   0   0   0]\n",
      " [  0   0   0  27   0   0]\n",
      " [  0   3   0   0  36  13]\n",
      " [  2   0   0   0   0 202]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55932203 0.96932515 0.97222222 1.         0.69230769 0.99019608]\n",
      "[0.86842105 0.98136646 1.         1.         0.70588235 0.88209607]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozge.bursa/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.68041237, 0.97530864, 0.98591549, 1.        , 0.69902913,\n",
       "       0.9330254 ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(clEnc.inverse_transform(y_test), clEnc.inverse_transform(y_pred), average=None))\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(clEnc.inverse_transform(y_test), clEnc.inverse_transform(y_pred), average=None))\n",
    "# F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(clEnc.inverse_transform(y_test), clEnc.inverse_transform(y_pred), average=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
